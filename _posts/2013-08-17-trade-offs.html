---
layout: post
title: Trade-offs
date: 2013-08-17 17:09:01.000000000 -04:00
categories: []
tags: []
status: publish
type: post
published: true
---
<h2>My data structure is better than yours</h2>
<p>Typically, when dealing with algorithms and data structures, a naive approach is to compare one data structure vs another or one algorithm vs another and talk about which one is better.  The problem with this approach is that there is usually a set of constraints in the mind of the person talking about how a particular data structure is better than another.  Those constraints are usually not a part of the spoken conversation.  </p>
<blockquote><p>I don't always query data...but when I do I only do it by a single key.</p>
</blockquote>
<p>Even worse is that a given data structure is always used without thinking about any others because that data structure is "better".  It is hard to have a conversation because a lot of statements are subjective statements: "it is better to use data structure X" or "algorithm Y sucks" - of course that could be true about bubble sort... ;)</p>
<h2>Trade-offs aka Know thy Complexities</h2>
<p>A better approach when dealing with data structures and algorithms is to discuss the trade-offs between two different data structures or two different algorithms.  The most common trade-offs to look at, in my opinion, are <a title="Time Complexity" href="http://en.wikipedia.org/wiki/Time_complexity">time complexity</a> (how fast the algorithm executes over the data) vs the <a title="Space Complexity" href="http://en.wikipedia.org/wiki/Space_complexity">space complexity</a> (how much additional memory is needed for the algorithm to execute).  There are other trade-offs which are valid - e.g. how difficult will it be to understand and maintain this algorithm.  However, the hard part about discussing these trade-offs is that they are much more subjective.  I like space vs time complexity trade-offs because they are much more objective and easier to quantify.  These complexities are measured in <a title="Big O Notation" href="http://en.wikipedia.org/wiki/Big_O_notation">Big O Notation</a>.  </p>
<p>A nice website that has quite a few data structures and algorithms and captures their space and time complexity is <a href="http://bigocheatsheet.com/">http://bigocheatsheet.com</a> - I took the 'Know thy Complexities' saying from this site.  Take a quick look at at sorting.  Most engineers know about bubble sort and merge sort.  If you look at the them and think about trade-offs, you start to see that the trade-off between bubble sort and merge sort is that merge sort has a faster time complexity and the trade-off is a larger space complexity.</p>
<h2>Know thy Environment</h2>
<p>Look at things in terms of the <a title="Asymptotic Complexity" href="http://en.wikipedia.org/wiki/Asymptotic_complexity">asymptotic complexity</a> of an algorithm on a data structure - e.g. sort - or an algorithm within a data structure - e.g. insert, delete.  Once you do that and recognize what the trade-offs are as you look at different algorithms and data structures, it will be much easier to pick one algorithm vs another or one data structure vs another once you know the limitations of the environment it will run in.  Knowing what operations you will perform - lookup by a single key (e.g. X = 5) vs lookup by a range of keys (e.g. X between 5 and 10 inclusive) might drive you to use a <a title="Binary Search Tree" href="http://en.wikipedia.org/wiki/Binary_search_tree">Binary Search Tree</a> instead of a <a title="Hash Table" href="http://en.wikipedia.org/wiki/Hash_table">Hash Table</a> where the trade-off is a slower time complexity for single key lookups - O(log n) vs O(1) - vs a faster time complexity for lookup by a range of keys - O(log n) vs O(n).  However, if 90%+ of your queries are single key lookups, you might choose a Hash Table so the single key lookups are faster knowing that the trade-off is a slower lookup by a range of keys, which rarely occurs in your environment.</p>
<h2>Know thy Technology</h2>
<p>Get to know the technology you are using.  What data structures are being used?  What algorithms are being used?  This will help in making better decisions in what might be causing slowdowns or how you might be able to speed things up - e.g. you will know how to go from linear growth to logarithmic growth by adding an index because you know what the underlying data structures that are involved with creating an index.  You will also know when adding an index will not cause faster lookups due to the number of collisions that occur within the data set (think of an index on a boolean field).</p>
<p> </p>
<p>Know your data structures and algorithms, their complexities and their trade-offs. Know your environment, its limitations, and what questions you will ask of your system. Know your technology and what data structures and algorithms it uses. Then you will be able to make better decisions and state in a quantitative manner why you made that decision.</p>
